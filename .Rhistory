scale_fill_gradient(name='Investor Number',low="cornsilk", high='chocolate4')+
ggtitle("Distribution of Chinese Venture Capital Investors") +
theme(axis.title.x=element_blank(),
axis.text.x=element_blank(),
axis.ticks.x=element_blank(),
axis.title.y=element_blank(),
axis.text.y=element_blank(),
axis.ticks.y=element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.border = element_rect(colour = "black", fill=NA),
panel.background = element_blank(),
legend.position="right")
china
points <- fortify(cnmap, region = 'NAME')
points2 = merge(points, prov_invstors, by.x='id', by.y='cn_prov_name', all.x=TRUE)
cnmap <- readOGR(dsn = "/Users/gao/Desktop/R/bou2_4p.shp", "bou2_4p")
cnmap@data <- cnmap@data[-899,] # remove the NA records
cnmap <- subset(cnmap, AREA > 0.005)
cn_prov_name <- na.omit(unique(cnmap$NAME))
en_prov_name <- c("Heilongjiang", "Neimenggu", "Xinjiang", "Jilin",
"Liaoning", "Gansu", "Hebei", "Beijing", "Shanxi",
"Tianjin", "Shaanxi", "Ningxia", "Qinghai", "Shandong",
"Tibet", "Henan", "Jiangsu", "Anhui", "Sichuan", "Hubei",
"Chongqing", "Shanghai", "Zhejiang", "Hunan", "Jiangxi",
"Yunnan", "Guizhou", "Fujian", "Guangxi", "Taiwan",
"Guangdong", "Hongkong", "Hainan")
# length(cn_prov_name) == length(en_prov_name)
prov_name <- data.frame(cn_prov_name, en_prov_name)
number_inv <- read.csv('investor.csv')
prov_invstors = left_join(prov_name, number_inv)
prov_invstors
number_inv <- read.csv('investor.csv')
prov_invstors <- left_join(prov_name, number_inv)
prov_invstors
number_inv <- read.csv('investor.csv')
prov_invstors <- left_join(prov_name, number_inv)
prov_invstors
library(mapdata)
library(sf) # classes and functions for vector data
library(raster) # classes and functions for raster data
library(spData) # load geographic data
library(spDataLarge) # load larger geographic data
library(dplyr)
library(tmap)
library(leaflet)
library(mapview)
library(ggplot2)
library(shiny)
library(stringr)
library(plotly)
library(ggplot2)
library(gapminder)
library(maptools)
library(dplyr)
library(ggplot2)
library(ggvis)
library(rgdal)
library("RColorBrewer")
library("scales")
# Sys.setlocale("LC_ALL", "chinese") Does not work
Sys.setlocale("LC_ALL","zh_CN.utf-8")
cnmap <- readOGR(dsn = "/Users/gao/Desktop/R/bou2_4p.shp", "bou2_4p")
cnmap@data <- cnmap@data[-899,] # remove the NA records
cnmap <- subset(cnmap, AREA > 0.005)
cn_prov_name <- na.omit(unique(cnmap$NAME))
en_prov_name <- c("Heilongjiang", "Neimenggu", "Xinjiang", "Jilin",
"Liaoning", "Gansu", "Hebei", "Beijing", "Shanxi",
"Tianjin", "Shaanxi", "Ningxia", "Qinghai", "Shandong",
"Tibet", "Henan", "Jiangsu", "Anhui", "Sichuan", "Hubei",
"Chongqing", "Shanghai", "Zhejiang", "Hunan", "Jiangxi",
"Yunnan", "Guizhou", "Fujian", "Guangxi", "Taiwan",
"Guangdong", "Hongkong", "Hainan")
# length(cn_prov_name) == length(en_prov_name)
prov_name <- data.frame(cn_prov_name, en_prov_name)
# Read the investor data
number_inv <- read.csv('investor.csv')
prov_invstors <- left_join(prov_name, number_inv)
prov_invstors
# cnmap_merged <- merge(cnmap, prov_invstors, by.x='NAME',by.y='cn_prov_name')
points <- fortify(cnmap, region = 'NAME')
points2 = merge(points, prov_invstors, by.x='id', by.y='cn_prov_name', all.x=TRUE)
# points2[is.na(points2)]<-0
# This fuckin worked
# low="cornsilk", high='chocolate4', aesthetics = "colour") max(points2$investors)
# scale_fill_gradient(name='Investor Number',low="cornsilk",mid='orange', high='chocolate4')+
#  scale_fill_distiller(palette = "Spectral",
# limits = c(min(points2$investors),8000))+
china <- ggplot()+
geom_polygon(aes(x=long,y=lat, group=group, fill=investors), data=points2, color='black') +
scale_fill_gradientn(colours=rev(brewer.pal(n = 8, name = "Spectral")),
breaks   = c(100,1500,5000,8000),
values   = rescale(c(100,1000,8000)))+
ggtitle("Distribution of Chinese Venture Capital Investors") +
theme(axis.title.x=element_blank(),
axis.text.x=element_blank(),
axis.ticks.x=element_blank(),
axis.title.y=element_blank(),
axis.text.y=element_blank(),
axis.ticks.y=element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.border = element_rect(colour = "black", fill=NA),
panel.background = element_blank(),
legend.position="right")
china
install.packages('grs')
install.packages('grf')
# Generate data.
n = 2000; p = 10
X = matrix(rnorm(n*p), n, p)
X.test = matrix(0, 101, p)
X.test[,1] = seq(-2, 2, length.out = 101)
# Train a causal forest.
W = rbinom(n, 1, 0.5)
Y = pmax(X[,1], 0) * W + X[,2] + pmin(X[,3], 0) + rnorm(n)
tau.forest = causal_forest(X, Y, W)
library(grf)
tau.forest = causal_forest(X, Y, W)
# Estimate treatment effects for the training data using out-of-bag prediction.
tau.hat.oob = predict(tau.forest)
hist(tau.hat.oob$predictions)
# Estimate treatment effects for the test sample.
tau.hat = predict(tau.forest, X.test)
plot(X.test[,1], tau.hat$predictions, ylim = range(tau.hat$predictions, 0, 2), xlab = "x", ylab = "tau", type = "l")
lines(X.test[,1], pmax(0, X.test[,1]), col = 2, lty = 2)
# Add confidence intervals for heterogeneous treatment effects; growing more trees is now recommended.
tau.forest = causal_forest(X, Y, W, num.trees = 4000)
tau.hat = predict(tau.forest, X.test, estimate.variance = TRUE)
sigma.hat = sqrt(tau.hat$variance.estimates)
plot(X.test[,1], tau.hat$predictions, ylim = range(tau.hat$predictions + 1.96 * sigma.hat, tau.hat$predictions - 1.96 * sigma.hat, 0, 2), xlab = "x", ylab = "tau", type = "l")
lines(X.test[,1], tau.hat$predictions + 1.96 * sigma.hat, col = 1, lty = 2)
lines(X.test[,1], tau.hat$predictions - 1.96 * sigma.hat, col = 1, lty = 2)
lines(X.test[,1], pmax(0, X.test[,1]), col = 2, lty = 1)
average_treatment_effect(tau.forest, target.sample = "treated")
tau.forest = causal_forest(X, Y, W, num.trees = 4000)
tau.hat = predict(tau.forest, X.test, estimate.variance = TRUE)
sigma.hat = sqrt(tau.hat$variance.estimates)
plot(X.test[,1], tau.hat$predictions, ylim = range(tau.hat$predictions + 1.96 * sigma.hat, tau.hat$predictions - 1.96 * sigma.hat, 0, 2), xlab = "x", ylab = "tau", type = "l")
lines(X.test[,1], tau.hat$predictions + 1.96 * sigma.hat, col = 1, lty = 2)
lines(X.test[,1], tau.hat$predictions - 1.96 * sigma.hat, col = 1, lty = 2)
lines(X.test[,1], pmax(0, X.test[,1]), col = 2, lty = 1)
install.packages('BART')
library(BART)
##simulate training data
sigma = .1
f = function(x) {x^3}
set.seed(17)
n = 200
x = sort(2*runif(n)-1)
y = f(x) + sigma*rnorm(n)
#xtest: values we want to estimate f(x) at
# this is also our prediction for y.
xtest = seq(-1,1,by=.2)
xtest = data.matrix(xtest, rownames.force = NA)
##plot simulated data
plot(x,y,cex=.5)
points(xtest,rep(0,length(xtest)),col="red",pch=16,cex=.8)
set.seed(14) #it is MCMC, set the seed!!
rb = wbart(x,y,xtest,nskip=500,ndpost=2000)
plot(x,y,cex=.3,cex.axis=.8,cex.lab=.7, mgp=c(1.3,.3,0),tcl=-.2,pch=".")
lines(xtest,f(xtest),col="blue",lty=1)
lines(xtest,apply(rb$yhat.test,2,mean),col="red",lwd=1.5,lty=2) #post mean of $f(x_j)$
qm = apply(rb$yhat.test,2,quantile,probs=c(.025,.975)) # post quantiles
lines(xtest,qm[1,],col="grey",lty=1,lwd=1.0)
lines(xtest,qm[2,],col="grey",lty=1,lwd=1.0)
legend("topleft",legend=c("true f","post mean of f","95% intervals"),
col=c("blue","red","grey"), lwd=c(2,2,2), lty=c(1,2,1),bty="n",cex=.5,seg.len=3)
## f and sigma
f = function(x){
10*sin(pi*x[,1]*x[,2]) + 20*(x[,3]-.5)^2+10*x[,4]+5*x[,5]
}
sigma = 1.0 #y = f(x) + sigma*z , z~N(0,1)
## simulate
n = 100 #number of observations
set.seed(99)
x=matrix(runif(n*10),n,10) #10 variables, only first 5 matter
Ey = f(x)
y=Ey+sigma*rnorm(n)
## run BART
burn=500;nd=5000
rbf = wbart(x,y,nskip=burn,ndpost=nd)
plot(rbf$sigma)
abline(v=burn,col="blue",lty=2)
lmf = lm(y~.,data.frame(x,y))
fmat = cbind(y,Ey,rbf$yhat.train.mean,lmf$fitted)
colnames(fmat) = c("y","Ey","BART-fit","lm-fit")
pairs(fmat,cex=.3,cex.axis=.5,cex.lab=.7, mgp=c(1.3,.3,0),
tcl=-.2,pch=".",cex.labels=.6)
cor(fmat)
dim(rb$yhat.train)
## [1] 2000 200
plot(rb$sigma)
sigma = .1
f = function(x) {x^3}
set.seed(17)
n = 100
x = sort(2*runif(n)-1)
y = f(x) + sigma*rnorm(n)
lmf = lm(y~x,data.frame(x,y))
sigest = summary(lmf)$sigma
cat("least squares estimate of sigma is: ", sigest ,"\n")
## least squares estimate of sigma is: 0.1742962
#density of sigma for sigma^2 ~ nu*lam/chisq_nu
sden=function(sv,nu,lam) {
dchisq(nu*lam/sv^2,df=nu)*(2*nu*lam)/sv^3
}
#lam val which puts sigest and sigquant quantile
lamval = function(sigest,sigquant,nu) {
qchi = qchisq(1.0-sigquant,nu)
lambda = (sigest*sigest*qchi)/nu #lambda parameter for sigma prior
lambda
}
#vary quantile and nu
qv=c(.5,.9,.99); nuv = c(3,200)
pg = expand.grid(qv,nuv)
names(pg) = c("quantile","nu")
pg
## quantile nu
## 1 0.50 3
## 2 0.90 3
## 3 0.99 3
## 4 0.50 200
## 5 0.90 200
## 6 0.99 200
#evaluate prior density of sigma for our 6 priors
sv = seq(from=.05*sigest,to=2*sigest,length.out=100)
pmat = matrix(0.0,length(sv),nrow(pg))
for(i in 1:nrow(pg)) {
pmat[,i] = sden(sv,pg[i,2],lamval(sigest,pg[i,1],pg[i,2]))
}
plot(range(sv),range(pmat),type="n")
lwv = rep(1,nrow(pg)); lwv[2]=3; ltyv = 1+floor(0:5/3)
for(i in 1:nrow(pg)) lines(sv,pmat[,i],col=i,lwd=lwv[i],lty=ltyv[i])
abline(v=sigest,col="red",lty=4,lwd=3)
abline(v=sigma,col="blue",lty=4,lwd=3)
fmat = matrix(0.0,n,nrow(pg))
for(i in 1:nrow(pg)) {
bf = wbart(x,y,sigquant=pg[i,1],sigdf=pg[i,2])
fmat[,i] = bf$yhat.train.mean
}
#more trees and more shrinkage => smoother f
bf2 = wbart(x,y,ntree=5000,k=5)
plot(x,y)
for(i in 1:nrow(pg)) lines(x,fmat[,i],col=i,lwd=lwv[i],lty=ltyv[i])
lines(x,bf2$yhat.train.mean,lty=1,lwd=3)
sigma = .1
f = function(x) {x^3}
set.seed(17)
n = 200
x = sort(2*runif(n)-1)
y = f(x) + sigma*rnorm(n)
#xtest: values we want to estimate f(x) at
# this is also our prediction for y.
xtest = seq(-1,1,by=.2)
xtest = data.matrix(xtest, rownames.force = NA)
##plot simulated data
plot(x,y,cex=.5)
points(xtest,rep(0,length(xtest)),col="red",pch=16,cex=.8)
set.seed(14) #it is MCMC, set the seed!!
rb = wbart(x,y,xtest,nskip=500,ndpost=2000)
plot(x,y,cex=.3,cex.axis=.8,cex.lab=.7, mgp=c(1.3,.3,0),tcl=-.2,pch=".")
lines(xtest,f(xtest),col="blue",lty=1)
lines(xtest,apply(rb$yhat.test,2,mean),col="red",lwd=1.5,lty=2) #post mean of $f(x_j)$
qm = apply(rb$yhat.test,2,quantile,probs=c(.025,.975)) # post quantiles
lines(xtest,qm[1,],col="grey",lty=1,lwd=1.0)
lines(xtest,qm[2,],col="grey",lty=1,lwd=1.0)
legend("topleft",legend=c("true f","post mean of f","95% intervals"),
col=c("blue","red","grey"), lwd=c(2,2,2), lty=c(1,2,1),bty="n",cex=.5,seg.len=3)
integrand <- function(x) 1 / ((x + 1) * sqrt(x)) integrate(integrand, lower = 0, upper = Inf)
integrand <- function(x) 1 / ((x + 1) * sqrt(x))
integrate(integrand, lower = 0, upper = Inf)
integrand <- function(x) 1 / ((x + 1) * sqrt(x))
integrate(integrand, lower = 0, upper = Inf)
q()
?ggplot
?plot
?ls
s <- 524
r <- 0.02
T <- 6/12
K <- s
sigma <- 0.20
callprice <- function(s, r, T, K, sigma) {
if (T == 0) return(max(0, s - K))
d1 <- (log(s / K) + (r + (1/2) * sigma^2) * T) / (sigma * sqrt(T)) d2 <- d1 - sigma * sqrt(T)
call <- pnorm(d1) * s - pnorm(d2) * K * exp(-r * T)
return(call)
}
callprice <- function(s, r, T, K, sigma) {
if (T == 0) return(max(0, s - K))
d1 <- (log(s / K) + (r + (1/2) * sigma^2) * T) / (sigma * sqrt(T)) d2 <- d1 - sigma * sqrt(T)
call <- pnorm(d1) * s - pnorm(d2) * K * exp(-r * T)
return(call)
}
log
callprice <- function(s, r, T, K, sigma) {
if (T == 0) return(max(0, s - K))
d1 <- (log(s / K) + (r + 0.5 * sigma^2) * T) / (sigma * sqrt(T)) d2 <- d1 - sigma * sqrt(T)
call <- pnorm(d1) * s - pnorm(d2) * K * exp(-r * T)
return(call)
}
callprice <- function(s, r, T, K, sigma) {
if (T == 0) return(max(0, s - K))
d1 <- (log(s / K) + (r + 0.5 * sigma^2) * T) / (sigma * sqrt(T)) d2 <- d1 - sigma * sqrt(T)
call <- pnorm(d1) * s - pnorm(d2) * K * exp(-r * T)
return(call)
}
callprice(s,r,T,K,sigma)
s <- 524
r <- 0.02
T <- 6/12
K <- s
sigma <- 0.20
callprice <- function(s, r, T, K, sigma) {
if (T == 0) return(max(0, s - K))
d1 <- (log(s / K) + (r + 0.5 * sigma^2) * T) / (sigma * sqrt(T))
d2 <- d1 - sigma * sqrt(T)
call <- pnorm(d1) * s - pnorm(d2) * K * exp(-r * T)
return(call)
}
print( callprice(s,r,T,K,sigma))
print( callprice(s,r,0,K,sigma))
price = 37
f <- function( price) callprice(s,r,T,K,sigma)
optimize((f(sigma)-37)^2), interval=c(0,50))
f <- function( sigma) callprice(s,r,T,K,sigma)
f <- function( sigma) callprice(s,r,T,K,sigma)
optimize((f(sigma)-37)^2), interval=c(0,50))
optimize((f(sigma)-37)^2, interval=c(0,50))
optimize((f,s=37, interval=c(0,50))
optimize(f,s=37, interval=c(0,50))
optimize(f,s=37, interval=c(0,50))
optimize(f,s=37, interval=c(0,1))
zt <- c(200,202,208,204,204,207,207,204	,202,	199	,201,	198,	200,	202,	203,	205,	207,	211)
acf(zt)
acf <- acf(zt)
acf$acf
zt.pacf <- pacf(zt)
zt <- c(200,202,208,204,204,207,207,204	,202,	199	,201,	198,	200,	202,	203,	205,	207,	211)
zt.pacf <- pacf(zt)
zt.pacf$pacf
attributes(zt.pacf)
zt.pacf$acf
pchisq(24.8779, df=8)
pchisq(24.8779, df=8, lower.tail = False )
pchisq(24.8779, df=8, lower.tail = F )
pchisq(25.4236, df=8, lower.tail=FALSE)
diag(2)
matrix(nrow = 2, ncol = 2)
matrix(9, nrow = 2, ncol = 2)
a <- matrix(9, nrow = 2, ncol = 2)
a
rbind(a, diag(2))
library(xts)
library(aTSA)
library(plyr)
library(lmtest)
library(stargazer)
acf.bartlett <- function( acf.result, n.obs, lags = 20, title='Autocorrelation Function',lower.lim = -0.25){
plot(acf.result$acf[1:lags],
type="h",
main=title,
xlab="Lag",
ylab="ACF",
ylim=c(lower.lim,1), # this sets the y scale to -1 to 1
las=1,
xaxt="n")
abline(h=0)
# Add labels to the x-axis
x <- c(1:lags)
y <- c(0:(lags-1))
axis(1, at=x, labels=y)
bartlett.bound(acf.result, n.obs )
}
bartlett.bound <- function( z, n ){
# Create a vector to store Bartlett's standard errors
bart.error <- c()
# Use a loop to calculate Bartlett's standard errors
bart.error[1] <- 1 /  n^0.5
for (k in 2:n) {
ends <- k
bart.error[k] <- ( (1 + sum( 2*z$acf[ 2:(ends) ] ^ 2) )  ) ^ 0.5 * (n^ -0.5 )
}
# Create upper bound of interval (two standard errors above zero)
upper.bart <- 1.96*bart.error
# Create lower bound of interval (two standard errors below zero)
lower.bart <- 1.96*-bart.error
# Add intervals based on Bartlett's approximations to ACF plot
lines((2:(length(upper.bart)+1)),upper.bart, lty=2, col="blue");
lines((2:(length(upper.bart)+1)),lower.bart, lty=2, col="blue")
}
## Load daily tweet data
df <- read.csv('trump_weekly_tweets.csv')
df <- df[2:(nrow(df)-1),]
total.length = nrow(df)
## Data for Forecast
forcast.values = df[ (nrow(df)-4) : nrow(df), ]
## ----------------------------------------
## Original Data Series Analysis
## ----------------------------------------
trump_weekly_series = matrix( df[1:(total.length-5),2], nrow = length(df[1:(total.length-5),2]))
rownames(trump_weekly_series) <- df[1:(total.length-5),1]
plot(trump_weekly_series,type='l')
weekly_ts <- xts(trump_weekly_series, order.by=as.Date(row.names(trump_weekly_series)))
plot(weekly_ts,main=NA)
hist(weekly_ts, main=NA,xlab = NA)
a.origin <- acf(weekly_ts)
acf.bartlett(a.origin, length(weekly_ts), title = NA)
plot( pacf(weekly_ts,lag=20), main = NA)
## ----------------------------------------
## Log weekly tweets
## ----------------------------------------
log_weekly_tweets = log( trump_weekly_series )
log_weekly_ts <- xts(log_weekly_tweets, order.by=as.Date(row.names(log_weekly_tweets)))
plot(log_weekly_ts,type='l', main=NA)
hist(log_weekly_tweets,breaks = 9,main=NA)
qqnorm(log_weekly_tweets)
qqline(log_weekly_tweets)
a <- acf( log_weekly_tweets,lag.max = 20 )
acf.bartlett(a, length(log_weekly_tweets),lags = 20, title = NA)
p <- pacf( log_weekly_tweets,lag.max = 20 )
plot(p, main = NA)
adf.log <- adf.test( log_weekly_tweets, nlag =7 )
a <- adf.test(weekly_ts,nlag = 7)
stargazer(adf.log)
## Null not rejected at lag 5. Hence, it might have a unit root.
## ----------------------------------------
## Differencing Log weekly tweets
## ----------------------------------------
lg.tw.d1 = diff(log_weekly_tweets)
setwd("~/Desktop/project_4601/trump_twitter_anlaysis")
library(xts)
library(aTSA)
library(plyr)
library(lmtest)
library(stargazer)
acf.bartlett <- function( acf.result, n.obs, lags = 20, title='Autocorrelation Function',lower.lim = -0.25){
plot(acf.result$acf[1:lags],
type="h",
main=title,
xlab="Lag",
ylab="ACF",
ylim=c(lower.lim,1), # this sets the y scale to -1 to 1
las=1,
xaxt="n")
abline(h=0)
# Add labels to the x-axis
x <- c(1:lags)
y <- c(0:(lags-1))
axis(1, at=x, labels=y)
bartlett.bound(acf.result, n.obs )
}
bartlett.bound <- function( z, n ){
# Create a vector to store Bartlett's standard errors
bart.error <- c()
# Use a loop to calculate Bartlett's standard errors
bart.error[1] <- 1 /  n^0.5
for (k in 2:n) {
ends <- k
bart.error[k] <- ( (1 + sum( 2*z$acf[ 2:(ends) ] ^ 2) )  ) ^ 0.5 * (n^ -0.5 )
}
# Create upper bound of interval (two standard errors above zero)
upper.bart <- 1.96*bart.error
# Create lower bound of interval (two standard errors below zero)
lower.bart <- 1.96*-bart.error
# Add intervals based on Bartlett's approximations to ACF plot
lines((2:(length(upper.bart)+1)),upper.bart, lty=2, col="blue");
lines((2:(length(upper.bart)+1)),lower.bart, lty=2, col="blue")
}
## Load daily tweet data
df <- read.csv('trump_weekly_tweets.csv')
df <- df[2:(nrow(df)-1),]
total.length = nrow(df)
## Data for Forecast
forcast.values = df[ (nrow(df)-4) : nrow(df), ]
## ----------------------------------------
## Original Data Series Analysis
## ----------------------------------------
trump_weekly_series = matrix( df[1:(total.length-5),2], nrow = length(df[1:(total.length-5),2]))
rownames(trump_weekly_series) <- df[1:(total.length-5),1]
plot(trump_weekly_series,type='l')
weekly_ts <- xts(trump_weekly_series, order.by=as.Date(row.names(trump_weekly_series)))
plot(weekly_ts,main=NA)
hist(weekly_ts, main=NA,xlab = NA)
a.origin <- acf(weekly_ts)
acf.bartlett(a.origin, length(weekly_ts), title = NA)
plot( pacf(weekly_ts,lag=20), main = NA)
## ----------------------------------------
## Log weekly tweets
## ----------------------------------------
log_weekly_tweets = log( trump_weekly_series )
log_weekly_ts <- xts(log_weekly_tweets, order.by=as.Date(row.names(log_weekly_tweets)))
plot(log_weekly_ts,type='l', main=NA)
hist(log_weekly_tweets,breaks = 9,main=NA)
qqnorm(log_weekly_tweets)
qqline(log_weekly_tweets)
a <- acf( log_weekly_tweets,lag.max = 20 )
acf.bartlett(a, length(log_weekly_tweets),lags = 20, title = NA)
p <- pacf( log_weekly_tweets,lag.max = 20 )
plot(p, main = NA)
adf.log <- adf.test( log_weekly_tweets, nlag =7 )
a <- adf.test(weekly_ts,nlag = 7)
stargazer(adf.log)
## Null not rejected at lag 5. Hence, it might have a unit root.
## ----------------------------------------
## Differencing Log weekly tweets
## ----------------------------------------
lg.tw.d1 = diff(log_weekly_tweets)
log_weekly_tweets
log_weekly_tweets[2] - log_weekly_tweets[1]
diff(log_weekly_tweets)[1]
